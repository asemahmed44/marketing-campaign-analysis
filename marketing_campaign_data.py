# -*- coding: utf-8 -*-
"""Marketing_Campaign_Data.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LHzBtU0pQfy91rnv1828DoFyK_Nk1q92
"""

import pandas as pd
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

data = pd.read_csv("/content/sample_data/marketing_campaign_data.csv")

print(data.head())

print(data.info())

print(data.describe())

print(data.isnull().sum())

plt.figure(figsize=(8, 5))
sns.histplot(data['Age'], kde=True, bins=10, color='skyblue')
plt.title('Age Distribution')
plt.xlabel('Age')
plt.ylabel('Frequency')
plt.show()

plt.figure(figsize=(8, 5))
sns.histplot(data['Income'], kde=True, bins=10, color='lightgreen')
plt.title('Income Distribution')
plt.xlabel('Income')
plt.ylabel('Frequency')
plt.show()

plt.figure(figsize=(8, 5))
sns.countplot(x='Response', hue='Marital_Status', data=data, palette='coolwarm')
plt.title('Response by Marital Status')
plt.xlabel('Response')
plt.ylabel('Count')
plt.show()

plt.figure(figsize=(8, 5))
sns.countplot(x='Response', hue='Education', data=data, palette='viridis')
plt.title('Response by Education Level')
plt.xlabel('Response')
plt.ylabel('Count')
plt.show()

#وزيع العمر والدخل: يمكن أن يوضح الفئات الأكثر انتشارًا بين العملاء.

"العلاقة بين الاستجابة والمتغيرات الأخرى: ستظهر لنا ما إذا كانت الحالة الاجتماعية أو مستوى التعليم يؤثران على استجابة العملاء للحملة"

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report, accuracy_score

# Convert categorical data to numerical using LabelEncoder
le_education = LabelEncoder()
le_marital_status = LabelEncoder()

data['Education'] = le_education.fit_transform(data['Education'])
data['Marital_Status'] = le_marital_status.fit_transform(data['Marital_Status'])

# Define features (X) and target (y)
X = data[['Age', 'Income', 'Education', 'Marital_Status', 'Purchases']]
y = data['Response']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Create and train the Decision Tree model
model = DecisionTreeClassifier(random_state=42)
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

print("Accuracy:", accuracy_score(y_test, y_pred) * 100, "%")
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, accuracy_score

svm_model = SVC(random_state=42)
knn_model = KNeighborsClassifier()
logreg_model = LogisticRegression(random_state=42)

# Train models
svm_model.fit(X_train, y_train)
knn_model.fit(X_train, y_train)
logreg_model.fit(X_train, y_train)

# Make predictions
svm_pred = svm_model.predict(X_test)
knn_pred = knn_model.predict(X_test)
logreg_pred = logreg_model.predict(X_test)

# Evaluate models
print("SVM Accuracy:", accuracy_score(y_test, svm_pred) * 100, "%")
print("\nSVM Classification Report:")
print(classification_report(y_test, svm_pred))

print("KNN Accuracy:", accuracy_score(y_test, knn_pred) * 100, "%")
print("\nKNN Classification Report:")
print(classification_report(y_test, knn_pred))

print("Logistic Regression Accuracy:", accuracy_score(y_test, logreg_pred) * 100, "%")
print("\nLogistic Regression Classification Report:")
print(classification_report(y_test, logreg_pred))

# Suppose we have new data (new_data) similar to the original data.
new_data = pd.DataFrame({
    'Age': [30],
    'Income': [55000],
    'Education': [1],  # Pre-coded
    'Marital_Status': [0],  # Pre-coded
    'Purchases': [5]
})

#Use the best model (e.g., KNN) to predict the response.
prediction = knn_model.predict(new_data)

print("Predicted Response:", prediction)

#After prediction using KNN or any other model
print("Predicted Response:", prediction)

print("Predicted Response: ", prediction)

#View the full output of each model.
print("SVM Prediction:", svm_pred)
print("KNN Prediction:", knn_pred)
print("Logistic Regression Prediction:", logreg_pred)

# Check the actual result
print("True Values:", y_test)

# Model evaluation
print("SVM Accuracy:", accuracy_score(y_test, svm_pred) * 100, "%")
print("KNN Accuracy:", accuracy_score(y_test, knn_pred) * 100, "%")
print("Logistic Regression Accuracy:", accuracy_score(y_test, logreg_pred) * 100, "%")

# View classification report
print("SVM Classification Report:")
print(classification_report(y_test, svm_pred))

print("KNN Classification Report:")
print(classification_report(y_test, knn_pred))

print("Logistic Regression Classification Report:")
print(classification_report(y_test, logreg_pred))

from sklearn.metrics import confusion_matrix

#   Confusion Matrix
print("SVM Confusion Matrix:")
print(confusion_matrix(y_test, svm_pred))

print("KNN Confusion Matrix:")
print(confusion_matrix(y_test, knn_pred))

print("Logistic Regression Confusion Matrix:")
print(confusion_matrix(y_test, logreg_pred))

